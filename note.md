# Paper Select

[![Status](https://img.shields.io/badge/status-Update_07.04_13:51-success.svg)]() 


## 目录

- [分类索引](#分类)
  - [Model](#Model)
  - [Agent](#Agent)
  - [Knowledge and Retrieval](#Knowledge-and-Retrieval)
  - [Alignment and Hallucination](#Alignment-and-Hallucination)
  - [Application](#Application)
  - [Pre-training and Instruction Fine-tuning](#Pre-training-and-Instruction-Fine-tuning)
  - [Survey](#Survey)

---

<a name='Model'></a>

## Model

### LLaMA: Open and Efficient Foundation Language Models
> **Institution:** Meta AI
>
> **Arxiv:** https://arxiv.org/pdf/2302.13971
>
> **Model:** https://huggingface.co/docs/transformers/main/model_doc/llama




### Llama 2: Open Foundation and Fine-Tuned Chat Models
> **Institution:** GenAI, Meta
>
> **Arxiv:** https://arxiv.org/pdf/2307.09288
>
> **Model:** https://huggingface.co/docs/transformers/main/model_doc/llama2

---

### Jamba: A Hybrid Transformer-Mamba Language Model

> **Institution:** AI21 Labs
>
> **Arxiv:** https://arxiv.org/pdf/2403.19887v1
> 
#### 背景
- **背景** 
  
    论文介绍了一种名为Jamba的新型大型语言模型，该模型基于一种新颖的混合Transformer-Mamba体系结构。Jamba将Transformer层和Mamba层以及专家混合（MoE）组件相结合，从而提供了改进的性能和更高的吞吐量，同时保持了可管理的内存足迹。所提出的7B基础的Jamba模型被设计成可以适配单个80GB GPU，且具备灵活的体系结构允许资源和目标特定的配置。
    
- **已有的工作**
    尽管Transformer作为大型语言模型的主导架构非常受欢迎，但存在两个主要问题：其对内存和计算资源的高需求限制了处理长上下文的能力；其缺乏单一摘要状态导致推理速度慢且吞吐量低。此外，尽管最近的状态空间模型（SSM）如Mamba在处理长距离关系上比递归神经网络（RNN）更有效，但仍然在性能上落后于同等规模的Transformer语言模型。

#### 核心贡献
- **提出了一个新型混合Transformer-Mamba体系结构的语言模型**
    - **挑战1：如何处理长上下文**
        Jamba采用混合Transformer层和Mamba层的方式，以及专家混合（MoE）组件，达到了处理长上下文的能力。其结构允许根据具体的硬件和性能需求调整Transformer与Mamba层的比例，以此来平衡内存使用、高效训练和长上下文处理能力的关系。

    - **挑战2：如何提升模型吞吐量同时保持较小的内存足迹**
        Jamba通过在某些MLP层应用MoE，增加了模型的容量而不增加计算需求（即活跃参数的数量保持可管理）。这种方法使Jamba能够训练极大规模的模型，并且表现出色，对于长上下文的评估有着更优的性能。

#### 实现与部署
Jamba模型在多个标准的语言模型基准测试及长上下文评估中表现出与Mixtral-8x7B相当的性能，该模型拥有类似数量的参数，并且也优于更大的Llama-2 70B模型。特别地，Jamba支持长达256K token的上下文——这是公开可用的生产级模型中支持的最长上下文长度。在长上下文评估中，Jamba在多数数据集上都比Mixtral表现更好，同时Jamba也极其高效，例如对于长上下文，它的吞吐量是Mixtral-8x7B的3倍。此外，即便是在超过128K token的上下文中，Jamba也能适配单个GPU（使用8位权重），这在类似规模的仅限于注意力的模型（如Mixtral-8x7B）中是无法实现的。

#### 总结
Jamba是基于混合Transformer-Mamba体系结构的新型大型语言模型，突破了处理长上下文的限制，并且通过应用专家混合（MoE）组件提高了模型吞吐量，同时保持了较小的内存足迹。此模型标志着在大型语言模型领域的一个新方向，并展示了高效训练与强大性能之间的可能平衡。

---

### PowerInfer-2: Fast Large Language Model Inference on a Smartphone

> **Institution:** Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University
>
> **Arxiv:** https://arxiv.org/pdf/2406.06282
>
> **Model:** https://huggingface.co/PowerInfer/TurboSparse-Mixtral



















---

<a name='Agent'></a>

## Agent

### Transforming Wearable Data into Health Insights using Large Language Model Agents

> **Institution:** Google LLC
>
> **Arxiv:** https://arxiv.org/pdf/2406.06464v1

#### 背景
- **背景** 
  
    文章介绍了可穿戴健康追踪设备和睡眠、运动对健康的重要性，以及从可穿戴设备数据中获得可操作的个性化洞察仍是一个挑战。尽管有关于身体活动和睡眠模式对健康影响的研究，但如何利用这些数据来提供针对个人健康问题的智能回应和见解并非易事，因为这需要在非监督的临床环境中对这些数据进行复杂的开放式分析。
    
- **已有的工作**
    以往的工作没有能够利用机器学习模型去解决从个人健康数据中得出洞察的挑战，因为这需要模型执行多个复杂的独立分析步骤，如考虑数据的可用性、选择优化指标、计算平均睡眠指标等。最近大型语言模型（LLM）显示出在要求推理和决策的复杂任务中生成语言的一些能力，但在分析个人健康方面的应用仍然很少。

#### 核心贡献
- **提出了一个名为个人健康洞察代理（PHIA）的LLM代理系统**
    - **挑战1：分析和解释可穿戴设备的行为健康数据的复杂性。**
        PHIA结合了先进的代码生成、网络搜索集成和ReAct代理框架来促进迭代推理，以应对数千个现实世界的健康查询。通过使代理能够自主分解复杂任务，于内部知识和外部工具的协同推理，并生成安全、可操作的洞察，PHIA增强了LLM代理在理解和分析时间序列行为健康数据方面的能力。

    - **挑战2：评估过程的广泛性和深度。**
        论文通过超过650个小时的人工评估和19名人工评注者对6000多个模型响应的评估，以及12000个模型响应的自动评估，证明了LLM代理在时间序列行为健康数据推理和深度健康洞察解释方面的卓越能力。与非代理LLM代码生成和仅文本数字推理方法相比，PHIA显示了更优越的能力。

#### 实现与部署
PHIA展现了在分析行为健康数据方面的准确性和应用可能性。具体评估结果显示PHIA能准确回答84%以上的事实性数值问题以及超过83%的由人群提出的开放式问题。这些结果证明了LLM代理在解释和分析个人健康数据方面的潜力。论文中也提供了高保真的合成可穿戴数据样本集合，及一个包含超过4000个封闭式和开放式问题的个人健康洞察评估数据集，用于自动化和人工评估。

#### 总结
本论文通过介绍名为PHIA的大型语言模型代理系统，成功地将可穿戴设备数据转化为个人健康洞察。PHIA结合了代码生成和信息检索工具，有效解决了从大量健康数据中派生个性化健康指导的挑战。通过广泛的人工和自动化评估，证明了这种方法在处理实际健康问题上的准确性和应用可能性。

---

### Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration

> **Institution:** Beijing Jiaotong University, Alibaba Group
>
> **Arxiv:** https://arxiv.org/pdf/2406.01014v1
>
> **Model: **https://github.com/X-PLUG/MobileAgent

#### 背景
- **背景**
  
    论文讨论了行动助理在移动设备操作任务中越来越受到欢迎的多模态人工智能应用场景。现有的多模态大型语言模型（MLLMs）受限于它们的训练数据，缺乏作为操作助手的有效功能。相反，基于MLLM的代理（agent），通过工具调用增强能力，在这一场景中逐渐得到应用。
    
- **已有的工作**
    现有的单代理架构难以有效解决移动设备操作任务中的两大导航挑战——任务进展导航和焦点内容导航。这是因为过长的令牌序列和交错的文本图像数据格式限制了性能。

#### 核心贡献
- **提出了一个多代理架构Mobile-Agent-v2**
    - **挑战1：任务进展导航**
        计划代理能够将冗长、交错的图像文本历史操作和屏幕总结转化为纯文本的任务进展，然后传递给决策代理。这样的压缩减少了上下文长度，使决策代理更容易导航任务进展。

    - **挑战2：焦点内容导航和反射能力**
        研究者设计了更新任务进展的记忆单元以及反射代理。记忆单元由决策代理用焦点内容更新，反射代理负责评估决策代理的操作是否符合预期，并在不符合预期时生成适当的补救措施。

#### 实现与部署
论文中的Mobile-Agent-v2在各种操作系统、语言环境和应用程序中进行了动态评估，实验结果表明与单代理架构相比，任务完成率提高了30%以上。此外，研究人员还实证验证了通过人工操作知识注入可以进一步提高Mobile-Agent-v2的性能。

#### 总结
Mobile-Agent-v2是一个多代理架构，能有效解决移动设备操作任务中的导航挑战，特别是任务进展和焦点内容的导航问题。通过引入三个专门的代理角色，相较于传统的单代理架构，显著提高了任务完成率。



---

<a name='Knowledge-and-Retrieval'></a>

## Knowledge and Retrieval



### Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies

> **Institution:** Duke University, AWS AI Labs
>
> **Arxiv:** https://arxiv.org/pdf/2406.06461v1

#### 背景
- **背景** 
    论文指出了目前评估大型语言模型（LLMs）推理策略时存在的一个主要问题。传统评估方法只注重性能指标，而忽略了由于额外计算资源带来的有效性提升，从而可能呈现出对策略效率的片面观点。

- **已有的工作**
    由于推理策略的计算要求不同，公平而全面地比较这些策略一直是一个挑战。例如，树状思考（ToT）策略需要分支出多个序列并纳入自我评估，使其比其他策略更为计算密集。只考虑性能指标的评估框架可能会忽视像计算成本这样的关键实用因素。

#### 核心贡献
- **提出了一个预算意识型评估框架**
    - **挑战1：如何平衡推理策略的性能和计算资源消耗？**
        论文介绍了一个将计算预算纳入不同推理策略性能测量的框架。这种预算意识型比较提供了一个更加平衡的视角，考虑了输出的质量和消耗的计算资源。研究发现复杂的推理策略往往并非由于算法灵巧而超越简单的基线，而是因为分配了更多的计算资源。对于一种像连贯思考自洽（CoT SC）这样的简单基线策略，只要与其他复杂方法拥有可比的计算资源，它常常能在性能和预算之间取得最佳平衡。

    - **挑战2：评估自我评估策略在不同模型和数据集上的表现**
        对自我评估策略进行深入探究，发现其性能实际上取决于模型和数据集。通过实证和理论证据，进一步调查了答案生成预算和评估预算两种特定类型的预算对性能的影响，并确认正确性预测代理的校准与利用自我评估的推理策略的成功强相关。

#### 实现与部署
该研究通过五种模型（包括GPT-4）对七种LLM推理策略进行了全面评估，跨越五个数据集。结果揭示了传统的评估指标常常忽略一个关键方面：通过额外的计算资源可获得的性能提升。这一观察得到了简单连贯思考自洽策略在有效性上可以匹敌，甚至超过更复杂策略的有力支持。此外，该论文还收录了对树状思考（ToT）和反思（Reflexion）进行预算分离的消减研究，并提出了新策略SC2以探索自我评估能力。

#### 总结
论文提出了一个考虑计算预算的LLM推理策略评估框架，并展示了简单策略在同等计算资源下可超越复杂策略的能力。通过揭示自我评估的重要性，为更加高效的预算利用和更有效策略的开发奠定了基础。



---

<a name='Application'></a>
## Application



### Octopus v2: On-device language model for super agent

> **Institution:** Stanford University
>
> **Arxiv:** https://arxiv.org/pdf/2404.01744v3

#### 背景
- **背景** 
    论文讨论了在边缘设备（如个人电脑或智能手机）上部署大型语言模型的挑战，如受限的存储能力和较低的推理速度，并指出了当前研究如使用小型LLM和LoRA方法来克服这些挑战。

- **已有的工作**
    尽管有尝试部署小型化的LLM到边缘设备，但这些工作主要侧重于提高设备上模型的推理速度，对于语言模型调用函数的能力仍然存在瓶颈。

#### 核心贡献
- **提出了一个高效的方法来增强在设备上的语言模型调用函数的准确性和延迟，实现了行业领先的结果**
    - **挑战1：如何在边缘设备上提高函数调用的精确性和减少延迟**
        通过对2B参数模型进行增强，该研究展示了如何利用标记化核心函数的名称和细粒度训练来解决这一挑战，使得在进行函数调用时比GPT-4表现更佳，可节省超过95%的上下文长度，在iPhone上使用时可实现相同电池下37倍的函数调用量，减少大约35倍的延迟。

    - **挑战2：如何训练语言模型以使其理解与特定功能令牌相关的意义**
        通过将函数描述集成到训练数据集中，模型能够学习这些特殊令牌的重要性。利用特殊令牌<nexa_end>作为早期停止标准，实现了快速准确的函数调用。

#### 实现与部署
论文借助谷歌Gemma-2B模型作为预训练模型并采用两种训练方法：全模型训练和LoRA训练。使用AdamW优化器和线性学习率调度器来进行训练。实验评估采用了详尽的基准测试方法，比较了模型生成函数调用的准确度和反应时间，并与顶尖的GPT-4和GPT-3.5模型进行了比较。通过RAG技术和Meta的FAISS进行语义搜索以增强函数调用描述的检索过程，显著提高了精确度和降低了延迟。评估研究了训练数据集的大小和模型训练方法对性能指标的影响，并发现即使是100个数据点的API也可实现98.095%的准确度，这表明在资源受限的情况下仍然能够保持较高的性能。

#### 总结
这篇论文解决了边缘设备上LLM的部署和功能调用效率问题，通过引入特殊的训练方法和减少推理时需处理的上下文量，显著提高了在设备上进行函数调用的准确率和降低了延迟，实验结果表明其对提升函数调用任务的性能具有显著影响。



---

### LLM in a flash: Efficient Large Language Model Inference with Limited Memory

> **Institution:** Apple
>
> **Arxiv:** https://arxiv.org/pdf/2312.11514v1
> 
#### 背景
- **背景**
    论文关注如何在内存有限的设备上高效地进行大型语言模型（LLM）的推理。现有的方法难以应对推理时的内存需求，尤其是在模型大小超过可用计算内存的情况下，这限制了在各种设备上部署模型的能力。

- **已有的工作**
    已有工作主要集中在压缩技术（如剪枝和量化）以及选择性执行等方面。但这些方法对减少从闪存到DRAM（动态随机存取存储器）的数据传输无能为力，且无法直接解决推理时闪存的高延迟问题。

#### 核心贡献
- **提出了一个高效的大型语言模型推理策略**
    - **挑战1：数据传输的高延迟**
        提出了减少数据负载、优化数据块大小和高效数据管理等策略，旨在减少与闪存I/O操作关联的延迟，提高数据传输的吞吐量，并优化已加载数据的管理。通过迭代地只传输必要的非稀疏数据从闪存到DRAM。

    - **挑战2：有限的DRAM容量**
        通过滑动窗口技术管理神经元数据，只保持用于最新一组输入tokens的神经元数据，从而可以有效利用内存资源。同时，我们驻留在RAM中保持转换器的注意力机制中的嵌入和矩阵，并且只有当需要时，才动态地加载FFN的非稀疏部分到DRAM中。

#### 实现与部署
提出的方法能让LLM在RAM内存可用空间是模型大小的一半甚至更少的设备上运行，相较于传统方法，我们在CPU和GPU上分别实现了4-5倍和20-25倍的推理速度提升。研究成果显著，远超过传统的加载方法，为资源受限环境中部署先进LLM提供了可能。

#### 总结
这份研究提供了一个创新且实用的解决方案，不仅能有效降低在内存受限设备上运行大型语言模型时的数据负载，还能显著提升推理速度，在实际应用中具有重要意义。